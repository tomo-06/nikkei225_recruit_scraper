import os
import requests
import pandas as pd
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# ======================
# 1. 環境設定
# ======================
load_dotenv()
api_key = os.getenv("GEMINI_API_KEY")
if not api_key:
    raise ValueError("環境変数 GEMINI_API_KEY が設定されていません。")

genai.configure(api_key=api_key)
model = genai.GenerativeModel("gemini-1.5-flash")

# ======================
# 2. 日経225銘柄リスト取得
# ======================
def get_nikkei225_list():
    url = "https://www.traders.co.jp/market_jp/nikkei225"
    res = requests.get(url)
    soup = BeautifulSoup(res.text, "html.parser")

    companies = []
    for row in soup.select("table tr"):
        cols = row.select("td")
        if len(cols) > 1:
            company_name = cols[1].get_text(strip=True)
            link_tag = cols[1].find("a")
            company_url = link_tag["href"] if link_tag else None
            companies.append({"company": company_name, "url": company_url})
    return companies

# ======================
# 3. 採用ページ候補を探す
# ======================
def find_recruit_page(base_url):
    candidates = []
    try:
        res = requests.get(base_url, timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")
        for a in soup.find_all("a", href=True):
            text = a.get_text(strip=True)
            href = a["href"]
            if any(keyword in text for keyword in ["採用", "新卒", "recruit", "careers"]):
                if href.startswith("http"):
                    candidates.append(href)
                else:
                    candidates.append(base_url.rstrip("/") + "/" + href.lstrip("/"))
    except Exception:
        pass
    return list(set(candidates))

# ======================
# 4. Geminiで人事部長プロフィール抽出
# ======================
def extract_hr_profile(text):
    prompt = f"""
    次の文章から、人事責任者のプロフィールをJSON形式で抽出してください。
    出力項目: name, role, career, message
    文章: {text[:2000]}  # 長すぎ防止
    """
    try:
        response = model.generate_content(prompt)
        return response.text
    except Exception:
        return None

# ======================
# 5. メイン処理
# ======================
def main():
    companies = get_nikkei225_list()
    print(f"=== 銘柄数: {len(companies)} ===")
    print(companies[:5])  # 先頭5件だけ確認

    results = []

    for c in companies[:5]:  # デバッグ用に5社だけ処理
        print(f"\n▶ {c['company']} : {c['url']}")
        if not c["url"]:
            continue

        recruit_pages = find_recruit_page(c["url"])
        print(f"  採用候補ページ: {recruit_pages}")

        for rp in recruit_pages:
            try:
                res = requests.get(rp, timeout=10)
                text = BeautifulSoup(res.text, "html.parser").get_text(" ", strip=True)
                print(f"  ページ文字数: {len(text)}")  # デバッグ用
                profile = extract_hr_profile(text)
                print(f"  Gemini出力: {profile}")
                results.append({
                    "company": c["company"],
                    "site": c["url"],
                    "recruit_page": rp,
                    "profile": profile
                })
            except Exception as e:
                print(f"  エラー: {e}")
                continue

    df = pd.DataFrame(results)
    df.to_csv("output.csv", index=False, encoding="utf-8-sig")
    print(f"✅ CSV出力完了: {len(df)}件")


if __name__ == "__main__":
    main()
